# A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications
[paper link](https://arxiv.org/pdf/2404.14809) 
| Year | Introduction                                                         | Research Field                 |
| ---- | ------------------------------------------------------------ | -------------------- |
| 2024 | This paper explores the application of Large Language Models (LLMs) to graph data analysis.          |  Large Language Models (LLMs)         |

## Methodology

### 1. Abstract
Compared to traditional graph learning models, LLMs have stronger generalization capabilities, can solve a variety of natural language processing(NLP) and multimodal tasks, and can answer arbitrary user questions and generate domain-specific content. The thesis presents a comprehensive survey of existing research on LLMs applied to graph data, summarizing state-of-the-art LLMs models for solving relevant graph analysis tasks as well as challenges and future directions. 

Specifically, the paper investigates three key issues in LLMs-based generative graph analysis (LLM-GGA): LLMs-based graph query processing (LLM-GQP), LLMs-based graph inference and learning (LLM-GIL), and LLMs-based applications.LLM-GQP focuses on the combination of graph analysis techniques and LLMs hints, including graph understanding and knowledge graph-enhanced retrieval; while LLM-GIL focuses on learning, reasoning, and representation over graphs. The paper also summarizes useful hints for handling different downstream graph tasks and provides LLM model evaluation, benchmark datasets/tasks, and in-depth strengths and weaknesses analysis. Finally, the paper explores unresolved issues and future directions in the exciting interdisciplinary research area of LLMs and graph analysis.

![image](https://github.com/user-attachments/assets/7b9bb290-86b6-4028-9450-63b0cd348799)

### 2. Method Description 
This paper proposes the Graph Description Language, a technique for converting graph data into sequential data for processing by pre-trained language models (LLMs). The technique preserves the structure and unique properties of the graphs and can be realized using a number of different graph description languages. These languages include textual descriptions, adjacency tables, edge lists, GML, GraphML, SQL, and others. In addition, graph structures can be represented using, for example, multimodal coding or story coding.

![image](https://github.com/user-attachments/assets/d97b349e-368b-4579-aaa6-ab28243a9b04)

![image](https://github.com/user-attachments/assets/f9b0bec3-cc0a-4aa1-8839-e65a762ceadf)

![image](https://github.com/user-attachments/assets/f272eb34-d16a-4b9f-8ef2-2f93089f5065)

### 3. Methodological improvements
The main advantage of Graph Description Languages over traditional graph processing techniques is that they can convert arbitrary shapes of graph data into sequential data, making them suitable for pre-training language models based on sequential data. At the same time, different results can be generated by using different graph description languages, so it is necessary to test multiple GDLs and select the best experimental results. And, unnecessary inference processes can be reduced by specifying the output form of the LMM.

### 4. Issues addressed 
The “graph description language” solves the problem that traditional graph processing techniques cannot be directly applied to pre-trained language models. By converting graph data into sequence data, the power of pre-trained language models can be utilized for complex graph analysis tasks. In addition, by using different graph description languages, the model performance can be optimized for different application scenarios.

## Experiments
This paper describes comparative experiments conducted by the authors for the task of graph structure understanding and provides a detailed explanation and analysis of the results. In these experiments, the authors used three approaches, manual prompting, self-prompting, and API call prompting, to evaluate the performance of language models (LLMs) for graph structure understanding. In addition, the authors explore the application of supervised fine-tuning methods in solving graph structure understanding problems.

  1. The authors conducted several experiments using manual prompts, including tasks such as calculating the number of nodes, degrees, connecting neighbors, simple paths, shortest paths, and maximum flows. The experimental results show that the LMM has preliminary graph structure understanding capabilities, but challenges remain on complex problems. For example, on structures such as chains and complete subgraphs, LMM exhibits false associations and performs poorly. Therefore, the authors concluded that further research is needed to improve the LMM's comprehension of complex graph structures.

  2. The authors assessed the LMM's comprehension on dynamic graph structures through a self-prompting approach. They designed a new cueing technique, Disentangled Spatial-Temporal Thoughts (DST2), which can enhance the LMM's comprehension of space and time. Experimental results show that the LMM has initial spatial and temporal comprehension, but the task becomes increasingly difficult as the graph size and density increase.

  3. The authors attempted a supervised fine-tuning approach, where some existing pre-trained models such as GPT-J and LLAMA are fine-tuned to improve their performance on the graph structure understanding task. This approach requires the use of external tool libraries to process the graph data and then combining them with language models. Although the method performs well in some cases, further research is needed to improve its effectiveness.
  
## Conclusion

### 1. Advantages of the Thesis
  1. The paper systematically describes methods for utilizing Pretrained Language Model (PLM) in graph learning tasks.
  2. For graph learning tasks, the methods are categorized into three types: augmenting Graph Neural Network (GNN) with PLM, using PLM as a predictor, and cue-based graph inference.
  3. Under each method, the specific implementation is described in detail and relevant experimental results are given.
  4. The authors also explore future directions, including how to deal with dynamic graphs and combine PLM and GNN to solve more complex problems.

### 2. Innovative points
  1. The method of using PLM to enhance GNN can improve the performance of the model, especially on the node classification task.
  2. The method of utilizing PLM as a predictor can be applied to different types of graph learning tasks by designing different hint statements.
  3. The cue-based graph inference approach can improve the performance of the model by guiding the model to produce a more rational inference process through the cue statements.

### 3. Future Works
  1. Further research can be conducted on how to combine PLM and GNN to solve more complex graph learning problems, such as predicting graph evolution.
  2. One can explore how to apply these methods on dynamic graphs for more practical application scenarios.
  3. Attempts can be made to use other pre-trained language models or improve existing models for better performance performance.    
