# Inconsistency in Conference Peer Review: Revisiting the 2014 NeurIPS Experiment
[paper link](https://arxiv.org/pdf/2109.09774) 
| Year | Introduction                                                         | Research Field                 |
| ---- | ------------------------------------------------------------ | -------------------- |
| 2021 | This paper reviews the 2014 NeurIPS experiment designed to explore the problem of inconsistency in conference peer review.         | Papers          |

## Methodology

Through their analysis, the authors found subjectivity in 50 per cent of the quality scores and no correlation between the quality scores of accepted papers and the number of citations after seven years. In addition, they tracked the fate of rejected papers and found a correlation between quality scores and impact in these papers. Thus, the authors concluded that the review process was effective for identifying low-quality papers, but less than ideal for identifying high-quality papers.

## Experiments
This paper describes an experiment conducted by the authors at the NeurIPS 2014 conference to test the consistency of the conference's peer-review process and explore its impact on the final results. The experiment involved two independent committees, each of which randomly selected 10 per cent of all papers submitted to the conference for review. Each committee consisted of two different groups of reviewers who were assigned to different papers. Each reviewer was asked to provide a quality score and a confidence score for each paper. All of these scores were used to calculate the acceptance rate for each committee and compared to the final conference acceptance rate.

The first comparison experiment was to check for inconsistency between the two committees. The authors used a correlation coefficient to measure the consistency of decision making between the two committees. The results showed that there were about 43 per cent inconsistent decisions between the two committees. The authors also conducted another simulation study to determine if subjective ratings could explain this inconsistency. The results showed that subjective ratings did explain some of the inconsistency, but not all of it.

The second comparison experiment examined the fate of rejected papers. The authors tracked the rejected papers and measured their quality scores and eventual citation impact. The results showed a weak correlation between the quality scores of the rejected papers and the final citation impact. This means that even though the rejected papers were not selected for the conference, they may be presented at other top conferences and receive high citations.  

## Conclusion

### 1. Advantages of the Thesis
  1. The paper delves into the issue of review consistency in machine learning conferences by revisiting the 2014 NeurIPS experiment and makes recommendations to improve the review process.
  2. The researcher used statistical methods and data analysis techniques to produce results of practical significance that inform subsequent research.

### 2. Innovative points
  1. The researcher used data simulation to explore the influencing factors of review consistency and verified them with real data.
  2. The researcher also proposes a new scoring method, in which different dimensions such as ‘quality’, ‘impact’ and ‘clarity’ are assessed separately, in order to improve the consistency and accuracy of the review.
  
### 3. Future Works
  1. Future research could further explore how to establish more stable and objective evaluation criteria to better reflect the actual contributions and capabilities of researchers.
  2. Meanwhile, the introduction of more evaluation dimensions, such as code quality and experimental design, can also be considered to assess the quality of research results more comprehensively.  



