# On the Meature of Intelligence
[paper link](https://arxiv.org/pdf/1911.01547.pdf) 
| Year | Introduction                                                         | Research Field                 |
| ---- | ------------------------------------------------------------ | -------------------- |
| 2019 |  This paper discusses how to define and assess the level of intelligence in AI and proposes a new definition of intelligence based on algorithmic information theory.        |  AI systems         |

## Methodology

### 1. Abstract
The authors argue that it is not sufficient to measure intelligence by simply comparing the performance of AI systems and humans on specific tasks, as skills are strongly influenced by a priori knowledge and experience. They therefore propose a new benchmark test for AI, the Abstract Reasoning Dataset (ARC), which is designed to be as close as possible to human innate intuition. The authors argue that the use of ARC allows for the measurement of a generic human-like fluid intelligence and allows for a fair comparison of intelligence levels between AI systems and humans.

### 2. Method Description 
The paper proposes a new formal definition of intelligence and evaluates this new definition through a new benchmark test called Abstraction and Reasoning Corpus (ARC). The definition is based on algorithmic information theory, which defines intelligence as skill acquisition efficiency, where key concepts include scope, generalised difficulty, prior knowledge and experience. In contrast to previous task-specific skill assessments, this approach emphasises the general adaptive capacity of intelligence, i.e. the ability to learn new skills in unknown tasks.

![image](https://github.com/user-attachments/assets/31bb21e4-30a9-4c27-a1e8-38b71a09401b)

### 3.  Methodological improvements
Traditional methods of intelligence assessment have focused on performance on specific tasks, such as board games or video games. However, these methods do not provide a good measure of the general adaptive capacity of intelligence. In contrast, the new definitions and benchmark tests take into account the various characteristics of intelligence more comprehensively, thus better reflecting the nature of human intelligence.

### 4. Issues addressed 
This thesis aims to address an important problem in the field of artificial intelligence: how to accurately define and measure intelligence. By proposing a new formal definition and a corresponding benchmark test, the thesis provides a more comprehensive and reliable method to assess the performance of intelligence. This helps to advance the development of AI technology so that it can better simulate and surpass human intelligence.

## Experiments
This article focuses on two different assessment methods for skills and abilities in the field of artificial intelligence, and explores the differences between them as well as the scope of application. Specifically, the article first introduces the skill-based assessment method based on task completion, i.e., measuring the performance of a machine by having it perform a specific task, such as AlphaGo in the game of Go. 

Then, the article discusses the ability-based assessment approach, which measures a machine's ability to perform on a variety of tasks in order to assess its level of intelligence. This approach involves designing a series of test tasks to cover different abilities and knowledge domains, such as language comprehension, image recognition, etc. Finally, the article also mentions attempts to apply psychological testing methods to the field of AI in order to assess a machine's capabilities more comprehensively.

Overall, by comparing and analysing different assessment methods, this article highlights that when assessing AI one should focus on ability rather than performance on a single task, and proposes a methodology for assessing a machine's ability using multiple test tasks. At the same time, this paper also points out the limitations of this approach, such as the inability to fully simulate human cognitive processes and the fact that test results may be affected by environmental factors. Therefore, in future research, we need to further explore how to more accurately assess the capabilities of AI in order to better promote the development of the field.  
