# Has Your Pretrained Model Improved? A Multi-head Posterior Based Approach
[paper link](https://arxiv.org/pdf/2401.02987) 
| Year | Introduction                                                         | Research Field                 |
| ---- | ------------------------------------------------------------ | -------------------- |
| 2024 |  This paper explores how pre-trained models can be evaluated more efficiently.         |   Pre-trained Models       |

## Methodology

### 1. Abstract
Traditionally, these models have been evaluated by fine-tuning downstream tasks, but this approach is inefficient and not effective enough. The authors therefore propose a new approach that utilises the relevant meta-features of each entity as a source of world knowledge and uses the entity representations generated by the model for evaluation. They propose the metric of the consistency of these representations with the meta-features as an effective metric for evaluating pre-trained models. The approach is validated in different domains, including relational datasets, large language models and image models.

### 2. Method Description 
This paper proposes a posteriori probability-based embedding evaluation metric for assessing the quality of embeddings generated by different models and identifying the best model. The main process of the algorithm includes: firstly, collecting a large number of entities and their rich meta-features; then for a given pre-trained model, generating an embedding dataset X and a corresponding feature set F; then dividing the entities into multiple clustered clusters by using a segmentation criterion; finally, calculating the a posteriori probability of each embedding in each cluster, and calculating an average logarithmic a posteriori probability (ALP) based on these probability values as the embedding quality assessment metrics.

Specifically, the authors employ a Gaussian mixture model (GMM) to estimate the probability distribution of the embeddings and select the optimal segmentation point by maximising the a posteriori probability. In addition, for high-dimensional embeddings and small-sample problems, the authors propose methods such as multi-head random subspace partitioning and regularisation matrix to improve the effectiveness of the algorithm.

![image](https://github.com/user-attachments/assets/b730a8fa-91bf-413e-af33-f0f5f6e7c5c0)

### 3. Methodological improvements
  1. It can effectively avoid the overfitting phenomenon and improve the model generalisation ability;
  2. It can deal with high-dimensional embedding and small sample problems, and is suitable for various practical application scenarios;
  3. It can directly use the existing segmentation criteria for embedding comparison, which is convenient and practical.

### 4. Issues addressed 
  1. How to evaluate the embedding quality generated by different models and determine the best model;
  2. How to deal with high-dimensional embedding and small sample problems and improve the effectiveness of the algorithm;
  3. How to use the existing segmentation criteria for embedding comparison, which is convenient and practical.

## Experiments
This paper describes a number of experiments conducted by the authors, including validation of a posteriori probabilistic embedding evaluation metrics using synthetic datasets, comparison of film embeddings generated by different models on the MovieLens dataset, and evaluation of image models in the Robustness library. The following is a detailed description of each experiment:

**Synthetic dataset experiments:** the authors designed three scenarios, each consisting of 10 Gaussian distributions, some of which are completely separated and others partially or completely overlapping. They evaluated different embedding algorithms using average log posterior (ALP) scores and accuracy, and observed that in the case of complete separation, the ALP score was 0 and the accuracy was 100%. However, in the case of partial overlap, the ALP score is negative and the accuracy is only 86.96 per cent. This indicates that this evaluation metric can effectively identify bad embeddings.

![image](https://github.com/user-attachments/assets/c2ab2509-c4a6-427c-8b28-9d9bfdd91910)

**Experiments on MovieLens dataset:** The authors used the MovieLens dataset for embedding relational models and compared the performance of three embedding methods, Word2vec, PDT and SASRec. The results show that PDT and SASRec perform best, while w2v single embedding outperforms w2v combined embedding. In addition, for year features, PDT and SASRec also encode year information more efficiently than other embedding methods.

![image](https://github.com/user-attachments/assets/a0d7919f-15ce-4324-b0c6-6f4edc240850)

**Robustness library experiments:** the authors evaluated the image models using datasets from the Robustness library. The results show that the Llama-2-70b model is the most effective because it captures location data better.

![image](https://github.com/user-attachments/assets/fab3ee58-b6ee-4e5e-ac1b-1b347d69b4cd)

**CLIP embedding experiments:** the authors used CLIP pre-trained models for embedding on the Breeds Hierarchy dataset and evaluated their performance using linear probes. The results show a strong correlation between the average log posterior and the linear probe performance, proving the validity of our evaluation metrics.  

![image](https://github.com/user-attachments/assets/33af00e2-dfba-46af-8e3c-2908df30082a)

## Conclusion

### 1. Advantages of the Thesis
  1. This paper presents a novel approach to evaluating pre-trained models that uses coherence between entity embeddings and their associated meta-features as a performance metric, rather than using an expensive and time-consuming downstream task for evaluation.
  
  2. This approach has been effectively tested in a number of domains, including relational datasets, NLP, and CV, and provides a more efficient and equally rigorous alternative for evaluating pre-trained models.

### 2. Innovative points
  1. The authors emphasise the quality of entity representations rather than focusing solely on the performance of downstream tasks. They argue that entity embeddings with their associated meta-features can be regarded as the knowledge base of the model's learning world.
  
  2. By using these meta-features for clustering entities and evaluating the posterior probabilities of mixed Gaussian distributions, the consistency and quality of entity embeddings can be assessed.
  
  3. In addition, the article provides a tree approach to interpret the meta-feature space, which is effectively tested on different datasets from a variety of domains. 

### 3. Future Works
Further future research should explore how to extend this evaluation method to suit more situations and apply it to real-world application scenarios. Also, the relationship between entity embeddings and meta-features and how to better utilise this information to improve the performance of pre-trained models needs to be explored in depth.  
