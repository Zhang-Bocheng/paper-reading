# TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization
[paper link](https://arxiv.org/pdf/2402.13249) 
| Year | Introduction                                                         | Research Field                 |
| ---- | ------------------------------------------------------------ | -------------------- |
| 2024 | This paper explores whether language model (LLM)-based summary generation has high factual consistency in topic-focused dialogue summary generation.          |  LArge Language Model (LLM)       |

## Methodology

### 1. Abstract
The researchers propose a new evaluation benchmark and provide annotations and detailed explanations of factual consistency at the binary sentence level. The experimental results show that existing LLMs suffer from a large number of factual errors in the dialogue domain that are not affected by model size. Furthermore, when LLMs are used as binary fact evaluators, they perform poorly and can be outperformed by even state-of-the-art specialised fact evaluation metrics. Finally, the researchers analysed the types of illusions in the summaries generated by the model and found a wide range of error types and distributions, all of which were better captured by the non-LLM-based metrics.

### 2. Method Description 
The TOFUEVAL benchmark test presented in this paper is constructed for the topic-focused dialogue summarisation task. The benchmark is constructed by randomly selecting documents from two publicly available dialogue summarisation datasets and generating multiple topic-relevant summaries using pre-trained language models (LLMs). During the evaluation process, the researchers selected four open-source models and one proprietary model for comparison. In addition, they provided human annotations to assess the factual consistency, relevance, and completeness of each summary.

![image](https://github.com/user-attachments/assets/6be09671-0a48-46a1-9881-86d403de94db)

### 3. Methodological improvements
Unlike traditional news summaries, dialogue summaries need to deal with informal and spoken text, which makes them more challenging. To address this issue, the researchers proposed the TOFUEVAL benchmark test, which simplifies the evaluation process by generating multiple sets of topic-related summaries using LLMs. In addition, they provided high-quality human annotations to better assess the quality of each summary.

### 4. Issues addressed 
The TOFUEVAL benchmark test addresses the issue of factual consistency in dialogue summaries. Dialogue summaries are prone to factual inconsistencies due to interactions and noise in the dialogue. By providing high-quality human annotations, researchers can more accurately assess the factual consistency of each summary and further improve the quality of dialogue summaries.

## Experiments
This paper focuses on the performance of using pre-trained language models (LLMs) as summary generators and factual consistency evaluators in factual consistency detection tasks. By comparing the performance of the different models, the authors analyse their strengths and weaknesses in different aspects and explore some possible reasons for this.

Firstly, the authors compare the error rates of LLMS in generating summaries. The results show that LLMs are prone to generate a large number of factual errors when generating summaries, especially when dealing with marginal topics. The authors further analysed the reasons for this phenomenon and found that when topics are rarely mentioned in the document, the model tries to rely on its own knowledge to make inferences, thus introducing unsupported information. In contrast, the GPT-3.5-Turbo model performs better in this regard, as it is better able to handle edge topics and avoid introducing unsupported information.

![image](https://github.com/user-attachments/assets/c64895cd-8c62-46fa-a71f-7f251ebf2492)

Second, the authors used LLMS as a factual consistency evaluator and compared it with non-LLM benchmark factual consistency measures. The results show that non-LLM benchmark fact consistency measures perform better than LLM in most cases. In addition, these benchmark methods offer faster inference, lower cost, and smaller GPU requirements. However, for some specific tasks, LLM may perform better, e.g., GPT-4 achieves better results on some datasets.

![image](https://github.com/user-attachments/assets/9530627e-35c9-4f2d-88d5-99272c2e636f)

Finally, the authors also analysed the ability of LLMS in identifying error types. The results show that non-LLM benchmark fact consistency measures perform better in capturing all error types. While LLM may perform better in identifying certain error types, better cue design is needed for improvement.

## Conclusion

### 1. Advantages of the Thesis
This paper proposes a new benchmark for assessing factual consistency of dialogue summaries, TOFUEVAL, and find experimentally that the use of Large Language Models (LLMs) as summarisers in the dialogue domain generates a large number of illusions and that existing factual consistency metrics have difficulty in detecting a wide range of illusion types. In addition, the study presents a dataset of factual consistency labels and interpretations based on expert annotations, which provides fundamental data for subsequent research.

### 2. Innovative points
  1. The main contribution of this paper is the introduction of TOFUEVAL, a benchmark for assessing factual consistency of dialogue summaries for topic focus, and a systematic experimental study.
  
  2. The study not only identifies the limitations of existing factual consistency metrics in the dialogue domain, but also reveals that non-LLM metrics have better performance advantages over LLM metrics.
  
  3. In addition, the study adopted an error classification system for error analysis, which provides ideas for further improving the automated factual consistency evaluation of summaries.

### 3. Future Works
Although the research in this paper has achieved some results, there are still some limitations. For example, human evaluators were not asked to annotate factual errors spanning multiple sentences due to the high complexity of the task; no distinction was made between the different levels of impact that different types of errors may have. Therefore, future work can further explore these issues and find solutions to improve the accuracy and efficiency of automated abstract factual consistency evaluation. Also, more metrics and techniques can be tried to address this issue.   
