# Implicit Diffusion: Efficient Optimization through Stochastic Sampling
[paper link](https://arxiv.org/pdf/2402.05468) 
| Year | Introduction                                                         | Research Field                 |
| ---- | ------------------------------------------------------------ | -------------------- |
| 2024 | This paper presents a new optimisation algorithm that can be used to optimise distributions defined by parametric stochastic diffusion.          |  Deep Learning        |

## Methodology

### 1. Abstract
The method modifies the output distribution of the sampling process through parameter tuning during the optimisation process. The authors present a general framework that allows both optimisation and sampling steps to be performed in a single loop. The approach is inspired by recent advances in bilayer optimisation and automatic implicit differentiation, and takes advantage of the view of sampling as optimisation on the space of probability distributions. The authors provide theoretical guarantees on the performance of the method and show the validity of their experimental results. They apply it to training energy models and fine-tuning denoising diffusions.

![image](https://github.com/user-attachments/assets/14e34aa1-ec60-4f7a-a283-cf2d245a3d27)

### 2. Method Description 
In this paper, a new optimisation algorithm, called ‘Implicit Diffusion Optimization (IDO)’, is proposed for solving the unsupervised pre-training problem of deep learning based models. The algorithm avoids solving high-dimensional integral problems directly by implicitly estimating the gradient, and can iteratively optimise both the objective function and the sample distribution. For a general form of the objective function, IDO uses some specific assumptions to derive an approximate gradient formula, and then combines this approximate gradient with the original objective function to form a new objective function. Then, models such as variational self-encoders or convolutional neural networks are used for pre-training.

![image](https://github.com/user-attachments/assets/6a18fa2b-aa92-42d2-ba54-3b2bdf6ba42f)

### 3. Methodological improvements
  1. It avoids solving high-dimensional integral problems directly, which reduces the amount of computation.
  2. It is possible to iteratively optimise the objective function and the sample distribution simultaneously, thus improving the effectiveness of pre-training.
  3. For general object function forms, IDO uses some specific assumptions to derive an approximate gradient formula, thus making the algorithm more general.

### 4. Issues addressed 
  1. The IDO algorithm proposed in this paper solves the problem of unsupervised pre-training based on deep learning models. During the pre-training process, the optimal parameters need to be obtained by maximising the ratio of the data distribution to some regularisation term. 

  2. In addition, the limited number of samples makes it difficult to obtain accurate gradient information. To solve these problems, this paper proposes the IDO algorithm, which can efficiently estimate the gradient and iteratively optimise the objective function and the sample distribution simultaneously, thus improving the pre-training.

## Experiments
This paper presents the authors' research on implicit diffusion algorithms and conducts several experiments to compare the performance of implicit diffusion with other methods. Specifically, they conducted the following two experiments:

The first experiment is **reward training of Langevin processes**. In this experiment, the authors considered the case where the latent function V(·, θ) is a logarithmic and exponential quadratic such that the output distribution is a Gaussian mixture model. They optimised the reward function R(x) = 1(x1> 0) exp-ix - µl2 , where µ ∈ Rd, as a demonstration of the ability of implicit diffusion methods to optimise even non-differentiable reward functions. They ran six sampling algorithms, including implicit diffusion algorithms over an infinite time horizon, all starting from an initial distribution N(0, Id) and making K= 5,000 steps. Through these experiments, the authors observed that the implicit diffusion method can efficiently optimise the sample distribution and has better performance compared to other methods.

![image](https://github.com/user-attachments/assets/a6751af5-17f2-4df0-a371-be5d43a02eb3)

The second experiment was reward training of denoising diffusion models. In this experiment, the authors applied implicit diffusion methods to fine-tune the rewards of pre-trained denoising diffusion models. They used an implicit diffusion algorithm with a finite time horizon and estimated the gradient using the inverse of the SDE. They report some samples generated by π*(θt) along with reward and KL scatter estimates. They also conducted experiments on three image datasets, MNIST, CIFAR-10, and LSUN, and showed results with different reward functions. Through these experiments, the authors found that the implicit diffusion method can effectively fine-tune the denoising diffusion model and produce good results on different datasets. 

![image](https://github.com/user-attachments/assets/5e0f9a29-eaa8-4119-ae3a-8acee2ca29a0)

  
