# TSSAT: Two-Stage Statistics-Aware Transformation for Artistic Style Transfer
[paper link](https://arxiv.org/pdf/2309.06004) 
| Year | Introduction                                                         | Research Field                 |
| ---- | ------------------------------------------------------------ | -------------------- |
| 2023 | This paper introduces a new art style transformation method, Two-Stage Statistics-Aware Transformation (TSSAT).          |          |

## Methodology

### 1. Abstract
Traditional art style transformation methods only consider global statistical information or local block information, without taking into account the details and diversity of the actual drawing process. As a result, such methods may ignore local art style patterns or include unwanted style image semantic information, thus deviating from the global style distribution. To address this problem, this paper proposes a TSSAT module that mimics the human drawing process, which first establishes a global style base by aligning the global statistical information of content and style features, and further enriches the local style details by exchanging the local statistical information of the style features at the pixel level, which significantly improves the stylisation effect. 

In addition, to further enhance the content and style representations, this paper introduces two novel loss functions: **attention-based content loss**, which better preserves the semantic relationships of content images, **and block-based style loss**, which focuses on increasing the local style similarity between styled and stylised images.

### 2. Method Description 
This thesis proposes a feature statistics-based image style transformation method, which consists of three main components: **an encoder, a two-stage statistically-aware transform module (TSSAT), and a decoder**. Among them, the encoder uses a pre-trained VGG-19 network to extract the feature vectors of the content and style images; the TSSAT module consists of two phases: global statistical matching and local statistical exchange, which are used to learn the global and local statistical information between the content and the style images and fuse them into a synthetic image containing rich details; the decoder uses the feature vectors generated by the TSSAT module to generate the final image.

![image](https://github.com/user-attachments/assets/3bcb8e51-ade6-4c32-8813-817f29356fc1)

### 3. Methodological improvements
Compared to traditional image style conversion methods, this method introduces the concept of feature statistics, which enables better preservation of the content structure of the original image and enhancement of its artistic style by capturing the global and local statistical information between the content and style images. In addition, the method employs a self-attention mechanism to further improve the expression of semantic relevance, thus achieving better style conversion results.

### 4. Issues addressed 
The method addresses some of the problems in traditional image style conversion methods, such as the difficulty in accurately capturing the semantic relationship between content and style images, and the over-reliance on pixel-level similarity in neural networks. By introducing feature statistics and self-attention mechanism, the method is able to enhance the artistic style of the original image more naturally while maintaining its content, thus improving the effect of image style conversion.

## Experiments
This paper focuses on TSSAT, a deep learning based image style migration method, and compares it with eight other popular methods. The experiments include quantitative and qualitative result analysis as well as component analysis and efficiency analysis.

For **the quantitative results**, the authors used perceptual distance and GELP as evaluation metrics to measure the content retention and style transformation performance of the different methods. In addition, a user survey was conducted to determine which stylisation results were preferred. The results showed that TSSAT achieved the best balance between content retention and style transitions and received the highest user preference score.

![image](https://github.com/user-attachments/assets/1dfab8a0-db90-406b-889c-5be4a8b29515)

In terms of **qualitative results**, the authors show sample images of the various methods and provide detailed explanations of their performance. For example, AdaIN often fails to capture sufficient stylistic patterns, while WCT suffers from severe structural distortion. In contrast, TSSAT not only accurately captures stylistic patterns, but also clearly preserves content structure.

![image](https://github.com/user-attachments/assets/b3be9121-b62c-4fcc-b0b6-0ceb6c373a5c)

In addition, the authors analysed the roles of different components of the TSSAT module and tested the speed and efficiency of the model. The results show that TSSAT has a speed comparable to previous work and can be further accelerated by increasing the block size of the local statistical exchange phase.  

## Conclusion

### 1. Advantages of the Thesis
  1. In this paper, a new style transformation module, TSSAT, is proposed, which significantly improves the style transformation effect by utilising feature statistics information to establish a global style base and further enrich local style details.
  2. And, the authors introduce attention-based content loss and block-based style loss to further improve content retention and style similarity. Experimental results show that the proposed method outperforms existing style conversion methods in terms of quality and efficiency.

### 2. Innovative points
  1. The main contribution of this paper is the proposed TSSAT module, which is able to simulate the basic structure and texture drawing during human painting, and then further enhance the fine-grained details.     2. The TSSAT module first aligns content and style features by global statistical information, and then for each local block of content features, finds its closest block of style features and exchanges their local statistical information, which avoids semantic information stored in the local features from being introduced into the transformation result.
  3. At the same time, this approach also allows flexible resizing of blocks for finer style pattern adjustment. And, the authors introduce attention-based content loss and block-based style loss to further improve content retention and style similarity.

### 3. Future Works
Future research can try to simplify this operation to improve the speed of conversion. In addition, more characteristic statistical information, such as variance and covariance, can be explored to further improve the conversion.  
