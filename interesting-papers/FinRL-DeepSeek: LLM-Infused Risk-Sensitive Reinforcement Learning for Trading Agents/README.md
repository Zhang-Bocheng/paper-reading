# FinRL-DeepSeek: LLM-Infused Risk-Sensitive Reinforcement Learning for Trading Agents
[paper link](https://arxiv.org/pdf/2502.07393v1) 
| Year | Introduction                                                         | Research Field                 |
| ---- | ------------------------------------------------------------ | -------------------- |
| 2025 |  This paper introduces a new risk-sensitive trading agent that combines reinforcement learning and Large Language Modelling (LLM)         | Reinforcement Learning (RL) & LLMs         |

## Methodology

### 1. Abstract
The approach extends the Conditional Value-at-Risk Proximal Policy Optimization (CPPO) algorithm and enhances its performance by adding risk assessment and trade recommendation signals generated by financial news. The authors tested this approach using the Nasdaq-100 index benchmark and conducted experiments using the FNSPID dataset and DeepSeek V3, Qwen 2.5, and Llama 3.3 language models. The results show that this risk-sensitive trading agent based on reinforcement learning and LLM performs well in the tests.

### 2. Method Description 
This paper presents three reinforcement learning based algorithms for stock trading; **Proximal Policy Optimisation (PPO), Conditional Value at Risk-Proximal Policy Optimization (CVaR-PPO) and LLM-infused PPO ( CPPO)**. Among them, PPO is a commonly used reinforcement learning algorithm that ensures the stability of policy updates by limiting the probability ratio; CVaR-PPO adds risk constraints to PPO to penalise high-loss trajectories; and CPPO combines financial news data with an LSTM model to generate a risk score for each stock and adjusts the trajectory returns to reflect market risk.

### 3. Methodological improvements
Compared with traditional stock trading algorithms based on technical indicators or fundamental analysis, the reinforcement learning algorithm proposed in this paper can better adapt to market changes, with higher prediction accuracy and better return performance. Meanwhile, CPPO also introduces the influence of financial news data to further improve the algorithm.

### 4. Issues addressed 
This paper aims to solve the problems that traditional stock trading algorithms cannot adapt to market changes, have low prediction accuracy and poor return performance. By introducing reinforcement learning algorithms and financial news data, this paper proposes a more intelligent, accurate, and stable stock trading algorithm, which is expected to provide investors with better investment advice and services.

## Experiments
This paper presents the authors' comparative experiments on the performance of different deep reinforcement learning models in stock trading. The experiment includes the following aspects:

1. **Early stop**: train the model using 10% LLM (language model) and observe its impact on PPO and CPPO.
2. **Long-term training**: extend the training history to 6 years and compare the performance of PPO, CPPO and DeepSeek V3.
3. **Higher training steps**: compare the performance of different models after 2 million training steps.
4. **Effect of LLM injection strength**: investigate the effect of stronger injections on the performance of PPO-DeepSeek and CPPO-DeepSeek by tuning the LLM injection strength parameter.
 
## Conclusion

### 1. Advantages of the Thesis
  1. This paper presents a novel hybrid RL-LLM trading agent that integrates financial news insights into action and risk levels and introduces LLM-based risk assessment scores and LLM-based trading recommendations.
  2. The study demonstrates the potential of LLM to go beyond standard sentiment analysis to extract features from news, implemented through well-designed cues.
  3. In addition, the study explores directions for future work such as optimising memory usage, reducing decision time scales and improving the quality of news signals.

### 2. Innovative points
  1. The main contribution of this paper is the proposal of an LLM-based risk assessment score that is extracted from the news.
  2. Also, the study incorporates trading recommendations from LLM, which enables trading agents to better respond to market changes.
  3. In addition, the study explores how to optimise memory usage, shorten decision time scales and improve the quality of news signals, providing ideas for future improvements.
 
### 3. Future Works
  1. Memory usage needs to be further optimised for longer training times and larger datasets;
  2. Trading performance could be improved by shortening the decision time scale;
  3. The quality of news signals needs to be improved to ensure better market performance.
These efforts will help to further improve the performance of trading agents and make them more applicable to real-world scenarios.    
