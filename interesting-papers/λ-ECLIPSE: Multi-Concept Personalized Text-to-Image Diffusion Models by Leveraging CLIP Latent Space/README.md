# λ-ECLIPSE: Multi-Concept Personalized Text-to-Image Diffusion Models by Leveraging CLIP Latent Space
[paper link](https://arxiv.org/pdf/2402.05195) 
| Year | Introduction                                                         | Research Field                 |
| ---- | ------------------------------------------------------------ | -------------------- |
| 2024 | This paper introduces a multi-subject personalised text-to-image diffusion model method called λ-ECLIPSE, which provides fast and efficient multi-subject-driven personalised text-to-image mapping by exploiting the latent space of pre-trained CLIP models.          |  Diffusion Model        |

## Methodology

### 1. Abstract
Compared to other P-T2I methods that rely on diffusion UNet models, λ-ECLIPSE does not require fine-tuning and has lower resource utilisation and better combinatorial alignment performance. In addition, λ-ECLIPSE demonstrates a unique multi-concept interpolation capability. Experimental results show that λ-ECLIPSE outperforms existing benchmarks while maintaining concept alignment performance and requires the use of only 34M parameters and is trained using only 74 GPU hours.

### 2. Method Description 
This paper proposes the λ-ECLIPSE method for multi-subject personalised text-to-image generation. The method combines the contrastive text-to-image strategy in ECLIPSE and introduces a novel image-text interleaved pre-training strategy that eliminates the need for explicit diffusion modelling. The efficient utilisation of the CLIP embedding space is mainly exploited. 

The goal of the method is to implement a P-T2I generation process for single and multiple topics, supporting edge graphs as a conditional guide. The problem formalisation and the UnCLIP stack design for λ-ECLIPSE are first described in detail. The image-text interleaved training method is then discussed in depth. This fine-tuning process allows λ-ECLIPSE to exploit the semantic correlation between the CLIP image and text embedding spaces while preserving the visual features of each topic.

![image](https://github.com/user-attachments/assets/ee922e33-e300-4433-b8cc-b05488377ca1)
 
### 3. Methodological improvements
Compared to traditional LDM-based P-T2I methods, λ-ECLIPSE employs a new mapping function, fθ, that processes textual representations (zy) as well as subject-specific visual representations (zxk) in order to estimate image embeddings that contain both textual cues and subject's vision (zˆx). By using a contrastive pre-training strategy, conceptual and structural consistency can be balanced. 

In addition, λ-ECLIPSE adds Canny edge maps as an additional input mode for better control of the generated results. During training, we temporarily disregard the additional condition (zc) to improve the quality of unconditional generation. This improves the stability of the model and extends its generalisation capabilities, producing good results without these controls.

### 4. Issues addressed 
**Learning the diffusion embedding space:** traditional LDM-based P-T2I methods require learning the diffusion embedding space, whereas λ-ECLIPSE omits this requirement through a contrastive pre-training strategy.

**Object visualisation:** by combining subject-specific visual representations with textual representations, λ-ECLIPSE is able to estimate target image representations more accurately, thus improving the quality of object visualisation.

**Better control:** by adding Canny edge maps as an additional input mode, λ-ECLIPSE provides better control to better drive the image generation process.

## Experiments
This paper focuses on the performance of the lambda-ECLIPSE model in an image generation task and compares it with other methods. The experiments include both quantitative and qualitative aspects, using several datasets and evaluation metrics to measure the performance of the models.

First, on the quantitative side, the authors compare lambda-ECLIPSE with several other methods (DreamBooth, BLIP-Diffusion, IP-Adapter, Kosmos-G, Emu2). They used DINO- and CLIP-based metrics to evaluate the performance of single-concept T2I tasks and tested multi-concept driven image generation using the ConceptBed benchmark. The results show that lambda-ECLIPSE has better combinatorial consistency while maintaining high conceptual consistency, whereas the other methods focus more on conceptual consistency. In addition, the authors conducted experiments on concept-specific fine-tuning and found that lambda-ECLIPSE improves CLIP-T scores while retaining high conceptual consistency.

![image](https://github.com/user-attachments/assets/1341d98e-0197-44a0-ba1a-7798b0349177)

![image](https://github.com/user-attachments/assets/76daf50f-94ca-4708-8935-61cde702f0c0)

Secondly, in terms of qualitative aspects, the authors showed samples of images generated by lambda-ECLIPSE and other methods and analysed them. The results show that lambda-ECLIPSE is able to capture complex text combinations well while maintaining high conceptual consistency. In contrast, other methods tend to overemphasise reference images or lead to conceptual dilution at the expense of combinatorial consistency.

![image](https://github.com/user-attachments/assets/6fba33ee-d06a-4e44-9626-6af5cc3abeb8)

In addition, the authors investigated Canny edge-controlled image generation and compared it to lambda-ECLIPSE and two other methods. The results show that lambda-ECLIPSE better preserves conceptual consistency while improving performance.
Finally, the authors also conducted multi-subject interpolation experiments to demonstrate that lambda-ECLIPSE is able to achieve smooth transitions in the CLIP potential space, thus extending its application.

## Conclusion

### 1. Advantages of the Thesis
  1. This paper presents a novel diffusion-free degree-of-freedom personalised text-to-image application approach to efficiently perform single-concept, multi-concept and edge-guided controlled personalised text-to-image tasks using the latent space of pre-trained CLIP models.
  2. The authors' proposed λ-ECLIPSE model implements multiple personalised text-to-image tasks simultaneously using a single model framework and is able to minimise resource utilisation.
  3. The study reveals the potential of λ-ECLIPSE in exploring and exploiting the space of smooth latent variables, which provides a new way to interpolate between different concepts and generate entirely new ones.
  4. Experimental results show that λ-ECLIPSE is able to achieve comparable performance to SOTA in single-concept, multi-concept and edge-guidance-controlled personalised text-to-image tasks while maintaining concept and combination alignment.

### 2. Innovative points
  1. This paper proposes a diffusion-free degree-of-freedom personalised text-to-image application method based on the CLIP model to efficiently perform single-concept, multi-concept and edge-guidance-controlled personalised text-to-image tasks by exploiting the latent space of the pre-trained CLIP model.
  2. The authors' proposed λ-ECLIPSe model implements multiple personalised text-to-image tasks simultaneously using a single model framework and is able to minimise resource utilisation.
  3. This study reveals the potential of λ-ECLIPSE in exploring and exploiting the space of smooth latent variables, which provides new ways to interpolate between concepts and generate entirely new ones.

### 3. Future Works
  1. Future research could further explore how to optimise the λ-ECLIPSE model to improve its performance and efficiency.
  2. Consideration could be given to combining the λ-ECLIPSE model with other techniques, such as adaptive learning rate tuning or more sophisticated architectural designs, to further improve its performance.
  3. In addition, ways to extend the λ-ECLIPSE model to support more personalised requirements, such as audio-to-image conversion, could be explored.   
