# ICE-GRT: Instruction Context Enhancement by Generative Reinforcement based Transformers
[paper link](https://arxiv.org/pdf/2401.02072) 
| Year | Introduction                                                         | Research Field                 |
| ---- | ------------------------------------------------------------ | -------------------- |
| 2024 | This paper describes a novel language model called ICE-GRT that enhances instruction context understanding by using reinforcement learning and human feedback.          | Transformers      |

## Methodology

### 1. Abstract
The model performs well in domain-specific tasks and does not compromise its performance in general tasks.The success of ICE-GRT relies on factors such as appropriate data, reward size scaling, KL control and advantage normalisation. ICE-GRT achieves state-of-the-art performance on both domain-specific tasks and general language tasks when compared to language models of the same size or even larger. Thus, the method brings important advances in the study of large-scale language models.

### 2. Method Description 

![image](https://github.com/user-attachments/assets/e3c9ba10-6eae-4939-b6d7-1a23e09e467d)

**ICE-GRT training data:** this dataset is derived from the ICE-Instruct model and carefully annotated with human feedback. Each prompt is given a diverse set of answers, which are generated by sampling from the model's output distribution. This approach ensures a comprehensive and diverse dataset, essential for robust model training.

**Reward Size Scaling:** In ICE-GRT, the scaling of the reward model is a key determinant of overall effectiveness and efficiency. Larger reward models can better capture complex environments and actions, which must accurately reflect human preferences and detailed task requirements in RLHF. In addition, larger reward scales help generalise across cues, which is particularly important for consistent performance across scenarios.

**KL Control:** KL control is a key mechanism in PPO, especially when training with human feedback. In this context, an important aspect of KL control is to control the differences between the Actor and Reference models. Also, ICE-GRT training includes a clipping mechanism to avoid potentially destabilising large updates. This ensures that changes in the value function are moderate and accurately reflect the actual improvements assessed by Critic. In addition, as an extra measure, KL reward tuning helps to keep the Actor model on the path defined for human feedback.

**Advantage Normalisation:** enhances learning stability and improves efficiency. It adjusts the dominance estimates to be more consistent and less variable. This is especially beneficial for RLHF, where human feedback can introduce unpredictable variation. Normalising dominance helps the model to focus on the most relevant learning signals, leading to faster and more stable convergence.

## Experiments
This paper describes the mixed dataset used by the authors in training a large language model and evaluates the performance and effectiveness of the model through several comparison experiments. Specifically, the authors conducted the following three comparison experiments:

**Comparison on public tasks:** the authors used the GPT-Fathom framework to evaluate the performance of the models on tasks with various proficiency categories (e.g., language comprehension, reasoning, etc.). The results show that the ICE-GRT 13B model performs well on a number of benchmark tests, outperforming other models, including LLama2, Vicuna13B, and LLama 30B.

**Manual annotation scoring experiment:** the authors designed a rigorous set of evaluation criteria focusing on manual annotation of different categories of responses to assess the model's capabilities. The results show that the ICE-GRT 13B model performs well in terms of clarity, accuracy, completeness, security, politeness, comfort, simplicity, and relevance, especially in terms of clarity and accuracy, which reached high scores of 98.1% and 97.0%.

**Intra-domain task comparison experiment:** the authors compared the ICE-GRT 13B model with two other models (LLama2 SFT13B and ICE-Instruct 13B), and the results show that the ICE-GRT 13B model outperforms the other two models on a number of key dimensions, especially on the metrics of Clarity, Accuracy, and Completeness, with an overall score of 95.5%, which is much higher than LLama2 SFT13B and ICE-Instruct 13B.

![image](https://github.com/user-attachments/assets/612f3db9-1292-4bb7-89e7-90d915b168a3)

Overall, these experimental results show that the ICE-GRT 13B model has excellent language comprehension and reasoning abilities, and achieves excellent performance on a wide range of tasks. The model is also able to handle domain-specific tasks effectively and performs well in manual annotation scoring experiments, demonstrating its ability to provide high-quality responses. Therefore, the authors conclude that the ICE-GRT 13B model is a very promising large-scale language model that can be applied to a variety of natural language processing tasks.

![image](https://github.com/user-attachments/assets/c5ae87bb-4ebd-4a81-93c5-91dc4d1274fa)

## Conclusion

### 1. Advantages of the Thesis
  1. The paper presents a new large-scale language model (LLM), ICE-GRT, that excels in processing domain-specific tasks with strong detailed analysis capabilities through a combination of reinforcement learning and human feedback.
  
  2. The authors also describe the key elements of the model's training process and provide a detailed analysis to help researchers better understand how the model works. And, the authors open-source ICE-GRT so that researchers worldwide can further explore and extend its findings.
  
### 2. Innovative points
  1. The authors used a combination of reinforcement learning and human feedback to improve the performance of the model in a specific domain and optimised the training process of the model by improving the quality of the training data, adjusting the size of the reward function, and controlling factors such as KL control.
  
  2. In addition, the authors provide some useful suggestions on how to train LLMs using this approach. 

### 3. Future Works
  The paper only deals with the development and application of a novel LLM, it would like to see more research efforts in this direction. It can expect more applications of AI techniques to domain-specific tasks, as well as more research on LLMs working with humans to solve problems.
