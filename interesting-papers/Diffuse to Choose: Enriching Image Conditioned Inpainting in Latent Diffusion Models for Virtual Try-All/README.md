# Diffuse to Choose: Enriching Image Conditioned Inpainting in Latent Diffusion Models for Virtual Try-All
[paper link](https://arxiv.org/pdf/2401.13795) 
| Year | Introduction                                                         | Research Field                 |
| ---- | ------------------------------------------------------------ | -------------------- |
| 2024 | This paper describes a novel image conditional interpolation model, called ‘Diffuse to Choose’, that efficiently balances fast inference with the preservation of high-fidelity details of a given reference term and ensures accurate semantic manipulation of the content of a given scene.          | Diffusion Model         |

## Methodology

### 1. Abstract
This approach is based on incorporating fine-grained features from the reference map directly into the latent feature mapping of the main diffusion model and further preserving the details of the reference term through perceptual loss. The authors conducted extensive testing on internal and public datasets and demonstrated that Diffuse to Choose outperforms existing zero-sample diffusion interpolation methods as well as few-sample diffusion personalisation algorithms such as DreamPaint.

### 2. Method Description 
The image conditional interpolation (Vit-All) task proposed in this paper is to integrate a product image into a user-specified image within a user-specified region while ensuring that the fine details of the product are preserved and harmoniously integrated with the target image. The task employs an image filling technique based on the diffusion model, which consists of the following steps:
  
  1. Noise injection and inverse process processing of the source image using the Stable Diffusion v1.5 model to obtain a feature representation similar to the reference image.
  2. Protecting the detail information in the reference image using an auxiliary U-Net network, which is incorporated into the main U-Net decoder by linear transformation, thus preserving the details of the reference image during the generation process.
  3. Different strategies of combining cue signals and the main U-Net encoder, such as Direct Add, FiLM and Cross Attention, are used to enhance the representation of fine details.
  4. Compare the feature mappings of the source and generated images by means of a perceptual loss function to ensure the alignment of essential features.

![image](https://github.com/user-attachments/assets/6e92fb80-9b05-407d-8373-b52ccc01abc3)

### 3. Methodological improvements
  1. In order to solve the information bottleneck problem in traditional PBE methods, the authors proposed the ‘Diffuse to Choose’ (DTC) method. Specifically, they use all CLIP patches instead of only tokens, use a larger image encoder DINOV2, and add an optimised loss similar to [7] to improve performance.
  
  2. In addition, DTC introduces the concept of cue signals, which are generated by creating zero images and inserting a reference image, and then combining the cue signals with the main U-Net encoder to further improve results.

![image](https://github.com/user-attachments/assets/0fa61e2b-f417-4de5-afd8-02c2e5162b28)

### 4. Issues addressed 
The Vit-All approach aims to address two key issues in the task of conditional interpolation of images: how to retain detailed information about the reference image and how to harmoniously blend it with the target image during the generation process. By using diffusion model-based techniques and improvements in the DTC method, the authors succeeded in improving the quality of the generated images and were able to better meet the user's expectations.

## Experiments
This paper focuses on Diffuse-to-Choose (DTC), a zero-sample image interpolation method based on image conditioning, and its improved version, Paint by Example (PBE), and conducts several sets of comparative experiments to verify its performance.

Firstly, for the dataset, the authors used a training dataset written by themselves and obtained a 1.2M sample dataset containing two types of products, wearables and furniture, through a series of filtering steps. Meanwhile, to ensure repeatability and accessibility, the authors also used a modified public dataset VITONHD-NoFace as the test dataset.

In terms of experimental details, the authors used Stable Diffusion V1.5 as the backbone network of the model and trained the model for 40 epochs. In terms of image resolution, the authors chose 512×512 and used DDPM to train at a constant learning rate of 1e-5. Additionally, the authors employed image enhancement techniques such as simple rotation and flipping, and used DDIM for generation during inference with a guidance scale of 5. In terms of guidance inputs, the authors stitched the reference image into a maximal rectangular bounding box.

Next, the authors conducted several comparison experiments to verify the performance of DTC and PBE:

For Paint by Example Ablations (PBE variant), the authors implemented several modifications to improve the performance of PBE. Specifically, the authors employed all CLIP patches instead of just the [CLS] token for image encoding, and employed DINOV2 to increase the capabilities of the image encoder. In addition, the authors introduced perceptual loss to promote consistency of basic features such as colour and certain textures. The results show that these modifications significantly improve the performance of PBE.

![image](https://github.com/user-attachments/assets/6e84cfb7-87f5-4a02-b03c-bb3e6ee907b4)

For the Diffuse-to-Choose Ablations (DTC variant), the authors explored several alternatives to insert a reference image directly into the masked source image. Specifically, the authors experimented with Canny edges and HED features, but found that they performed slightly worse than using the reference image directly. In addition, the authors explored different ways to merge the cue signals with the main U-Net, including adding them directly, using the affine transform layer FiLM, and fusing them at different time points. The results show that adding cue signals directly is the most effective.

![image](https://github.com/user-attachments/assets/74c66058-b54f-48c2-ba5b-87cefae00c7d)

In a comparison with other methods, the authors compared DTC with DreamPaint and PBE. While DreamPaint required a small number of samples for fine-tuning, DTC performed well without any samples. In addition, the authors conducted a subjective human survey, which showed that DTC was comparable to DreamPaint in terms of the similarity of the filled area to the reference image and contextual blending. 

![image](https://github.com/user-attachments/assets/c9bef603-66ee-4cd3-bc3e-6b6bae86596d)

## Conclusion

### 1. Advantages of the Thesis
  1. The paper proposes a novel image conditional diffusion interpolation model called ‘Diffuse to Choose’, which aims to provide a more efficient solution for Virtual Try-All. Compared with existing GAN or AR/VR techniques, the model has higher generality and applicability, and is capable of fast real-time inference with zero samples. 
  2. In addition, the model is able to efficiently handle complex scenes and product details while maintaining harmony between the product's identity features and the surrounding environment.

### 2. Innovative points
  1. More accurate and detailed product insertion is achieved by injecting fine information from the reference image into the primary U-Net decoder using a secondary U-Net encoder, as well as by adding pixel-level cues in the void regions and processing them using a shallow convolutional network.
  2. In addition, the paper employs a perceptual loss function to further improve the consistency of basic features such as colour. 

### 3. Future Works
The introduction of pose conditions can be considered to solve this problem in future research. In addition, the model can be further extended to other areas such as virtual furniture design.   
