# GITA: Graph to Visual and Textual Integration for Vision-Language Graph Reasoning
[paper link](https://arxiv.org/pdf/2402.02130.pdf) 
| Year | Introduction                                                         | Research Field                 |
| ---- | ------------------------------------------------------------ | -------------------- |
| 2024 | This paper presents a framework called GITA that combines visual graph structures with textual information for performing general graphical reasoning tasks.         |  LLMs         |

## Methodology

### 1. Abstract
The authors present a dataset called Graph-based Vision-Language Question Answering (GVLQA), which is the first visuo-linguistic dataset for general graphical reasoning purposes. Experimental results show that GITA performs better in general graphical reasoning ability compared to mainstream language models and validates the effectiveness of visual graph layout enhancement and pre-training.

### 2. Method Description 
The Graph to Image-Txt Assistant (GITA) proposed in this paper is a graph comprehension method based on visual textual reasoning. It consists of four key components: a task-independent graph visualizer (V), a graph descriptor (D), a task-specific querier (Q), and a visual language model reasoner (R). 

First, structured graphics are converted to visual graphics by the graph visualizer and textual descriptions of the structured graphics are generated by the graph descriptor. Then, task-specific query statements are generated by the querier based on the task requirements and textual descriptions. Finally, taking the visual graphs and task-specific query statements as input, the visual language model justifier generates natural language answers.

![image](https://github.com/user-attachments/assets/ec1e55ad-5e32-4c7e-b594-8e3c2d401ca1)

### 3. Methodological improvements
In the graph visualizer, a combination of basic image styles and customizable graph-related image styles is used in order to balance consistency and variety. Also, a k-hop subgraph sampling strategy is used for large graphs to improve clarity. The graph descriptor, on the other hand, is designed with a series of uniform and structured templates covering a variety of different types of graph configurations. The querier refines and enriches the graph descriptions based on task requirements and textual descriptions to form task-specific query statements.

![image](https://github.com/user-attachments/assets/49659932-82a7-43a6-a295-96721fb011ee)

### 4. Issues addressed 
The approach proposed in this paper aims to address the problem of graph comprehension based on visual textual reasoning. Specifically, it can be used to evaluate the capabilities of visual language models, as well as to help the models acquire basic graph comprehension and reasoning capabilities. In addition, this paper introduces the GVLQA dataset, which can be used to train and evaluate graph understanding models based on visual textual reasoning.

## Experiments
This paper focuses on the task of visual graph-based instruction reasoning, and evaluates and compares the proposed GITA model through a series of experiments. Specifically, the article includes the following four parts:

Experiments: the details of the experiments are first introduced, including comparing with the popular LLM baseline on the GVLQA-BASE dataset, comparing the model performance under different parameter scales, and investigating the effect of visual graphics alone.

![image](https://github.com/user-attachments/assets/b5ee42a6-e074-4736-9aa8-e20f50f31463)

Comparison Experiment 1: The authors compare GITA with popular LLM benchmarks (e.g., GPT-4 Turbo, LLAMA2-7B/13B, and Vicuna-7B/13B), and the results show that GITA outperforms these benchmarks under the same settings. In addition, the authors found that mainstream open-source LLMs or VLMs lack basic graphical inference capabilities, while current SOTA closed-source LLMs or VLMs exhibit better zero-sample performance.

Comparative Experiment 2: The authors compare the performance of VLM/LLMs with different parameter sizes in graphical inference tasks, and the results show that increasing the model size improves graphical inference. However, for some tasks, increasing the model size may not always be beneficial, possibly due to the fact that fine-tuning the LoRA adapter alone does not adequately address the modal mismatch.

Comparative Experiment 3: The authors explored the respective capabilities and complementary relationships between visual and textual modalities in graphical reasoning tasks. The results show that visual and textual modalities can complement each other in many situations, and that using either modality alone can lead to performance degradation. The visual modality has an advantage in capturing loops and graph properties, while the textual modality is more adept at handling sequence-related problems.

 ![image](https://github.com/user-attachments/assets/9642f6e3-2041-4df0-bfa0-739934460fbf)

## Conclusion

### 1. Advantages of the Thesis
  1. A new approach to graph reasoning is proposed: the GITA framework effectively improves instruction-based graph reasoning performance by transforming graph structures into customized visual images and combining visual information with natural language models.

  2. The first visual language quiz dataset supporting general graph reasoning capabilities is built: the GVLQA contains 526K instances covering seven representative graph reasoning tasks, aiming to comprehensively evaluate the structural base graph reasoning capabilities of VLMs and LLMs.
  
  3. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of the GITA framework and the importance of layout enhancement strategies.

### 2. Innovative points
  1. Graph Visualizer: for generating visual graphs.
  
  2. Graph Descriptor: for generating textual descriptions of graph structures.

  3. Task-Related Questioner: to organize the description and requirements of the current task to form instruction prompts.
  
  4. Visual Language Model (VLM): performs visual language graph reasoning.

### 3. Future Works
  1. **Broader application scenarios**: the GITA framework can be applied to graph inference tasks in more domains, such as recommender systems, social network analysis, and knowledge graph inference.
  
  2. **Model optimization and improvement**: By further tuning and optimizing the components of the GITA framework, as well as exploring other visual enhancement strategies, its performance and applicability can be improved.
  
  3. **Dataset Extension and Diversification**: By adding more task types and richer data samples, the GVLQA dataset can better reflect real-world graphical reasoning needs, thus facilitating the research and development of visual language models.
