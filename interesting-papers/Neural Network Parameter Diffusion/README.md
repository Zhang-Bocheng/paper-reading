# Neural Network Parameter Diffusion
[paper link](https://arxiv.org/pdf/2402.13144) 
| Year | Introduction                                                         | Research Field                 |
| ---- | ------------------------------------------------------------ | -------------------- |
| 2024 | This paper describes a method for generating high-performance neural network parameters using a diffusion model.          |  Diffusion Model        |

## Methodology

### 1. Abstract
The authors use an autoencoder and a standard latent diffusion model to extract trained network parameters from their latent representations and synthesise them into random noise via the diffusion model. The new representations are then generated and can be used as new network parameters after being output by the autoencoder decoder. Experimental results show that the performance of the models generated by this method is comparable or better than that of the trained network on a variety of architectures and datasets, and at a very low cost. In addition, the authors found that the generated models did not memorise the trained network.

### 2. Method Description 
The paper presents a method called neural network parameter diffusion (p-diff), which aims to generate high-performance neural network parameters from random noise. The method consists of two processes: parameter self-encoder and generation. First, a subset of a set of trained high-performance models is selected and spread into one-dimensional vectors. Then, an encoder is introduced to extract the latent representations of these vectors, while a decoder is responsible for reconstructing the latent representations back into parameters. Next, a standard latent diffusion model is used to train the synthesis of latent representations from random noise. After training is complete, the p-diff is used to generate new parameters via the following chain: random noise → reverse process → train decoder → generate parameters.

![image](https://github.com/user-attachments/assets/fe173b45-2364-4efa-af6c-fd212d498f3e)

### 3. Methodological improvements
Compared to synthesising new parameters directly, this method can significantly reduce the memory cost by first converting the parameters into potential representations before diffusion processing, especially when the parameter dimensions are extremely large. In addition, to enhance the robustness and generalisation of the self-encoder, the method introduces random noise in the input parameters and potential representations.

### 4. Issues addressed 
The generation of neural network parameters is often difficult and time-consuming, as it requires significant computational resources and expertise. Traditional parameter synthesis methods often only generate low performance parameters or require extensive manual tuning. In contrast, the neural network parameter diffusion proposed in this method can generate high-performance neural network parameters quickly and efficiently, thus improving the performance and efficiency of neural networks.

## Experiments
This paper focuses on the use of diffusion models to generate high-quality parameters in image classification tasks, and several sets of comparative experiments are conducted to verify their effectiveness and feasibility. Specifically, the authors conducted the following sets of comparison experiments:

**Performance comparison experiments**: the authors used several datasets such as MNIST, CIFAR-10/100, ImageNet-1K, STL-10, Flowers, Pets, and F-101, as well as models with different architectures such as ResNet-18/50, ViT-Tiny/Base, and ConvNeXt-T/B, to conduct performance comparison experiments. The results show that in most cases, the method can obtain similar or even better results compared to the two baseline methods, proving the effectiveness and generality of the method.
![image](https://github.com/user-attachments/assets/53a85103-1116-4de7-bd7d-2e2bae942335)

**Whole parameter generation experiments**: to further validate the effectiveness of the method, the authors extended the training data collection strategy from collecting only the batch normalised parameters to the whole model parameters. They used CIFAR-10 as an example and showed the details of two small networks (MLP-3 and ConvNet-3). The results show that the method can efficiently learn the distribution of high-performing parameters over a wider range of parameters and generate new models with better performance.
![image](https://github.com/user-attachments/assets/c0215d25-0fd2-407d-af6e-04271b864b68)

**Ablation research experiments**: the authors conducted several Ablation research experiments to better understand the features and mechanisms of the method. For example, they explored aspects such as the effect of the number of trained models on stability, the effect of applying p-diff on different depths of the batch normalisation layer, the role of noise enhancement on stability and generalisation ability, and consistency with other optimisers such as Adam and AdamW.
![image](https://github.com/user-attachments/assets/1a2e5d89-3dbe-4ec5-9ece-e7db5c9ffc38)

**Whether it is just a memory experiment**: finally, the authors also designed some experiments to explore whether the method simply remembers samples of the original model. They proposed a similarity metric of intersection and union ratio (IoU) and performed four sets of comparisons: (1) similarity between the original models; (2) similarity between the p-diff models; (3) similarity between the original models and the p-diff models; and (4) maximum similarity (nearest-neighbours) between the original models and the p-diff models. 
![image](https://github.com/user-attachments/assets/4d9bc486-272b-45b6-b1c7-6ea21fcb7049)

The results show that the maximum similarity between even the closest original and generated models is lower than the similarity between the original models, suggesting that p-diff can generate new parameters that are different from the original models. In addition, the generated models have higher diversity and better performance than the operations of fine-tuning and adding noise. 

## Conclusion

### 1. Advantages of the Thesis
  1. **Able to generate high-quality neural network parameters**: through experiments, the neural network parameters generated using the diffusion model can achieve the same performance level as the original training data or even better.

  2. **Can accelerate the training process of neural network**: Since the generated neural network parameters can be directly used for training, the training time of neural network can be greatly reduced.

  3. **Good generality**: the method can be applied not only to image classification tasks, but also extended to other visual tasks, such as object detection, semantic segmentation and so on.

### 2. Innovative points
  1. Using a 1D convolutional layer instead of a fully connected layer: considering the difference between the visual signals and the neural network weights, the method uses a 1D convolutional layer to process the neural network parameters, which improves the generation effect.

  2. Adding noise: adding noise to the input and potential representations helps to improve the generation.

  3. Different types of neural network parameters were explored: in addition to the normalisation layer, the generation effects of different types of parameters such as linear, convolutional and skip layers were investigated.

### 3. Future Works
  1. **Exploring more diffusion models**: currently the method only uses a unidirectional diffusion model, and in the future, more complex multidirectional diffusion models can be tried to improve the generation effect.

  2. **Incorporate adaptive learning rate**: since the parameters of the generated neural network may be different from the original training data, incorporating adaptive learning rate can be considered to optimise the training process.

  3. **Applying to larger datasets**: The method has been tested on several datasets and can be applied to larger datasets in the future to evaluate its performance in real-world applications.   
