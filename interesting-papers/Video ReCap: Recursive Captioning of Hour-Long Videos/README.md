# Video ReCap: Recursive Captioning of Hour-Long Videos
[paper link](https://arxiv.org/pdf/2402.13250) 
| Year | Introduction                                                         | Research Field                 |
| ---- | ------------------------------------------------------------ | -------------------- |
| 2024 | This paper presents a recursive video captioning model called Video ReCap that can process video inputs of varying lengths and output video captions at multiple levels.          | Large Language Model (LLM)         |

## Methodology

### 1. Abstract
While traditional video captioning models can only handle short clips of a few seconds, Video ReCap is able to handle complex videos up to several hours in length, exploiting synergies between different video layers to process videos in an efficient manner. The model uses a recursive video language architecture and a hierarchical training approach that allows starting from clip-level subtitles describing atomic actions, gradually focusing on paragraph-level descriptions, and eventually generating summaries that summarise the whole video. In addition, the authors propose the Ego4D-HCap dataset for augmenting long-distance video summaries in the Ego4D dataset. This recursive model not only enables flexible generation of subtitles at different levels, but can also be applied to other complex video comprehension tasks such as video quizzing on EgoSchema.

![image](https://github.com/user-attachments/assets/0ca00a36-e513-4535-bb40-65a702b5656c)

### 2. Method Description 
This paper proposes a video text generation model called Video ReCap for generating video titles with different hierarchical structures. The model contains three main components: **a video encoder, a video-verbal alignment module and a recursive text decoder**. The video encoder uses a pre-trained temporal Sformer network to extract dense spatio-temporal features of the video sequence and classify them into short clips or global features as needed. The video-language alignment module combines video features with a pre-trained language model to learn a fixed number of embedding vectors that are processed together in a subsequent text decoder. The recursive text decoder is a pre-trained GPT2 model for generating video titles with different hierarchical structures.
![image](https://github.com/user-attachments/assets/0e3facab-96eb-49fe-abc3-46da27f6380d)

### 3. Methodological improvements
In order to solve the problems of large differences in video lengths, data imbalance, and exploiting the relationships between different hierarchies, three strategies are proposed in this paper:

  1. A layer-by-layer training strategy based on the laws of human perceptual action organisation, which starts training with low-level clip titles and then gradually increases the high-level titles.
  2. Generate a large amount of pseudo-labelled data using Large Language Model (LLM) as additional training samples to improve the generalisation ability of the model.
  3. A TimeSformer network is used as the video encoder and a GPT2 network is used as the recursive text decoder, while the Adam optimiser and cosine scheduling strategy are used for training.

### 4. Issues addressed 
The Video ReCap model proposed in this paper can effectively solve the multilevel hierarchy problem in video title generation, and can generate video titles with different hierarchical structures, including short clip titles, intermediate paragraph descriptions, and whole video summaries. In addition, the performance of the model can be further improved by using the pseudo-labelled data generated by LLM.

## Experiments
This paper focuses on a recursive model designed for the task of describing and summarising long sequences of video, and its performance is verified through a series of comparative experiments. Specifically, the paper compares experiments in the following three areas:

**Comparison of baseline methods:** the authors compare their model with three text-based baseline methods (BLIP2, BLIP2+GPT3.5, LaViLa+GPT3.5) and two video-based baseline methods (LaViLa, LaViLa+FLAN-T5). The results show that these baseline methods perform poorly in both description and summarisation tasks, whereas the fully fine-tuned methods are able to significantly improve performance.

**Comparison of model variants:** the authors also compared two different model variants, the Video ReCap model that uses a shared encoder but separates the decoder and language alignment modules, and the Video ReCap-U model that has all levels of shared parameters. The results show that the Video ReCap model performs better than the other model because it can utilise different levels of linguistic input to generate more accurate descriptions and summaries.

**Importance of LLM supervision:** finally, the authors also investigated the role of generating pseudo-labelled data using pre-trained Large Language Models (LLMs) for improving model performance. The results show that the performance of the model is further improved in the case of using pseudo-labelled data.

![image](https://github.com/user-attachments/assets/7a649db4-6b93-44ac-b96d-0a9dc52adf2b)

## Conclusion

### 1. Advantages of the Thesis
  1. The paper introduces Video ReCap, a recursive video captioning model that can produce hierarchical captions for videos spanning diverse temporal granularities.
  2. The incorporation of a curriculum learning scheme inspired by human psychology and an LLM-based supervision strategy enhances the model's efficacy in tackling the hierarchical video captioning problem.
  3. The authors also release the curated Ego4D-HCap dataset, which can be used to analyze ongoing progress in video understanding research.

### 2. Innovative points
  1. The authors propose a recursive video captioning model called Video ReCap that can generate hierarchical captions for videos of different temporal granularities.
  2. They incorporate a curriculum learning scheme inspired by human psychology and an LLM-based supervision strategy to enhance the model's performance.
  3. The authors also release the curated Ego4D-HCap dataset, which can be used to analyze ongoing progress in video understanding research.

### 3. Future Works
  Future work includes exploring real-time caption generation, interactive video understanding, and video-based dialogues. The authors suggest that their proposed model could have potential applications in these areas. Additionally, they mention that further research could focus on improving the model's efficiency and scalability.  
