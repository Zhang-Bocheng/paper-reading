# Evaluating Very Long-Term Conversational Memory of LLM Agents
[paper link](https://arxiv.org/pdf/2402.17753) 
| Year | Introduction                                                         | Research Field                 |
| ---- | ------------------------------------------------------------ | -------------------- |
| 2024 |  This thesis explores the effectiveness of Large Language Models (LLMs) evaluated in the context of very long conversations.         | Large Language Models (LLMs)         |

## Methodology

### 1. Abstract
While existing long-term open-domain dialogue research focuses on no more than five chat sessions in context, this thesis introduces a machine-human pipeline for dialogue using LLM-based agent architectures and character and temporal event graphs, augmented by shared and reaction images. The dialogue generated by this pipeline is validated and edited by a human annotator to ensure long-term consistency and grounding in the event graph. 

Based on this pipeline, the authors collected a dataset called LOCOMO, which contains each dialogue containing 300 turns and an average of 9K tokens, covering up to 35 conversations. The authors present a comprehensive evaluation benchmark for measuring the long-term memory capabilities of the model, including question and answer, event summary, and multimodal dialogue generation tasks. Experimental results show that LLM has challenges in understanding long conversations and understanding causal dynamics over long time scales. Using strategies like long context LLM or RAG can provide improvements, but these models still lag far behind human performance.

### 2. Method Description 
The LO-COMO model proposed in this paper is an AI chatbot based on a pre-trained language model (LLM). The model consists of four main components: personality statements, temporal event graphs, virtual agent architecture, and human verification and editing. Each component has a specific function and purpose. 

Firstly, personality statements are implemented by expanding initial utterances from the MSC dataset into full personalised statements. These statements typically contain information about elements such as personal goals, past experiences, daily habits, and interpersonal relationships. Next, time-event graphs are used to capture each agent's real-life experiences and reflect them in the dialogue. Then, the virtual agent architecture uses two modules, Reflected Response and Image Shared Response, to enhance the multimodal dialogue experience. Finally, the human validation and editing phase is tasked with editing the dialogue to remove long-term inconsistencies and remove or replace irrelevant images, while ensuring that the event graph matches the content of the dialogue.

![image](https://github.com/user-attachments/assets/2a4799c8-8560-44bb-9ef5-1e3b990a36d6)
 
### 3. Methodological improvements
The main contribution of this research is the proposal of a new chatbot model that is not only capable of processing natural language input, but also of using temporal event graphs to reflect real-life situations during the dialogue. In addition, the design of the virtual agent architecture gives it a strong multimodal dialogue capability, which can better simulate real-world interaction scenarios. Finally, human intervention during the human verification and editing phase helps to improve the quality and consistency of the dialogue.

### 4. Issues addressed 
This paper aims to address the limitations of current AI chatbots in terms of dialogue quality and multimodal interaction. By introducing four components, namely personality statements, temporal event graphs, virtual agent architecture, and human verification and editing, the model can more effectively simulate real-world dialogue scenarios and provide a more humane communication experience.

## Experiments
This paper presents a detailed experimental analysis of three tasks based on dialogue generation models: question and answer task, event summary task and multimodal dialogue generation task. Specifically, the study uses dialogues from the LOCOMO dataset as training samples, employs a variety of models for different tasks, and evaluates model performance by metrics such as precision and recall.

In the question and answer task, the authors compared three types of models: a baseline model based on limiting the length of the context, a long text model with an extended context window, and a model based on retrieval augmented generation (RAG). The results show that the long text model performs better in comprehending longer dialogues, but is prone to hallucinations, while the RAG-based model works better in some situations. In addition, temporal reasoning and open domain knowledge problems were among the most challenging scenarios.

![image](https://github.com/user-attachments/assets/3910ad41-ac02-4733-99e7-f36697114873)

In the event summarisation task, the authors compared two types of models: a baseline model based on limiting the context length and a long text model with an extended context window. The results show that the incremental summary generation approach improves model performance, while the long text model does not show better performance. In addition, the authors found that LMM models have some difficulties in understanding and processing dialogues over long time spans.

![image](https://github.com/user-attachments/assets/956cbad7-aebf-45fb-ad5f-2401a7da5f0a)

In a multimodal dialogue generation task, the authors compared three types of models: a baseline model that relies only on previous dialogue rounds, a baseline model based on observations and event summaries, and a model generated based on retrieval enhancement. The results show that introducing observations into training significantly improves model performance, while increasing the length of dialogue history leads to a decrease in MM relevance scores. Furthermore, the authors show that the RAG approach can mitigate this decline.

## Conclusion

### 1. Advantages of the Thesis
  1. The paper presents a dataset based on a human machine pipeline collection, LOCOMO, and develops an evaluation framework to assess the model's ability to handle very long dialogues. The dataset contains 50 high-quality very long dialogues, each consisting of 300 rounds and 9,000 tokens spanning up to 35 sessions. The experimental results show that the Long-term Memory Model (LLM) has difficulty in understanding long-term narratives and establishing causal relationships between events. 

  2. In addition, the paper explores the benefits and limitations of using hybrid human machine-generated data, as well as the exploration of multimodal behaviour.
     
  3. Finally, the thesis suggests directions for future research, such as improving the model's memory capacity, improving the model's understanding of long-distance causality, and exploring more multimodal dialogue behaviours.

### 2. Innovative points
  1. The methodological innovation of the thesis is that by constructing a dataset based on a human machine pipeline collection, LOCOMO, and developing an evaluation framework to assess the model's ability in processing very long dialogues. The dataset is characterised by (1) unique personalities, (2) causally linked events in the timeline of an individual's life, and (3) reflective and reactive mechanisms to respond to image sharing and reactive behaviours based on dialogue history.
  
  2. In addition, the paper proposes a comprehensive evaluation framework that includes a question and answer task, an event graph summary task, and a multimodal dialogue generation task to assess the model's ability to handle very long dialogues.
     
### 3. Future Works
  1. improving the model's memory ability, such as using better pre-training techniques or introducing external knowledge bases;
  
  2. improving the model's understanding of long-distance causality, such as using more complex event graph structures or introducing common-sense reasoning;
 
  3. exploring more multimodal dialogue behaviours, such as emotional expressions and intonation changes.
 These research directions will help to further improve the model's ability in dealing with very long dialogues and provide better support for practical applications.  
