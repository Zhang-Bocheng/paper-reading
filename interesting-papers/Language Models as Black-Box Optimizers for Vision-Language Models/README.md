# Language Models as Black-Box Optimizers for Vision-Language Models
[paper link](https://arxiv.org/pdf/2309.05950) 
| Year | Introduction                                                         | Research Field                 |
| ---- | ------------------------------------------------------------ | -------------------- |
| 2023 | This paper describes a black-box optimisation method for training visual language models (VLMs).          | Large Language Models (LLMs)         |

## Methodology

### 1. Abstract
As many VLMs rely on proprietary data that cannot be fine-tuned using white-box methods, the approach aims to optimise VLMs with natural language cues, avoiding the need to access model parameters, feature embeddings or output logits. The researchers used a dialogue-based large-scale language model (LLM) to search for optimal textual cues and an automated ‘hill-climbing’ process to evaluate the performance of current cues and allow the LLM to improve them based on textual feedback. 

Experimental results show that the method outperforms the white-box continuous cueing method (CoOp) by an average of about 1.5% on 11 datasets, including ImageNet, in a challenging one-shot image classification setting. In addition, the method outperforms human-designed and LLM-generated cues, and textual cues generated by this strategy are found to be not only more interpretable, but also black-box transferable between different VLM architectures.

![image](https://github.com/user-attachments/assets/7cd35e1e-84ae-4225-a8b2-4d78f1bc527f)

### 2. Method Description 
The paper presents a chatbot-based natural language cue optimisation algorithm for image classification tasks. The algorithm requires a chatty LSTM (Long Short-Term Memory) model and an evaluation function (e.g., accuracy). Exploration is achieved by iteratively resetting and resetting at random, and exploitation is achieved by using the LSTM model to adjust the previous result in each iteration. Ultimately, the algorithm returns the cue with the highest score.

Specifically, the algorithm first randomly samples an initial set of cues from a text corpus (e.g., LAION-COCO [57]). Then, it follows the classical stochastic hill-climbing framework, which balances the ‘exploration’ and ‘exploitation’ processes to prevent ChatGPT from falling into local optima. At each iteration, the top k best cues are selected based on the scores in the cue set and passed to ChatGPT, then new cues are added to the cue set and the process is repeated until the maximum number of iterations is reached. Finally, the algorithm returns the cue with the highest score.

![image](https://github.com/user-attachments/assets/1d7ece8b-d754-448c-980d-581ca8db5cb6)

![image](https://github.com/user-attachments/assets/28d59f68-b1ce-409c-9eb3-37fd0eca5aae)

### 3. Methodological improvements
The algorithm introduces two key mechanisms: **exploration and exploitation**. Exploration is achieved by randomly resetting and resetting to avoid falling into local optima. Exploitation, on the other hand, is performed by conversing with ChatGPT multiple times to further improve the quality of the hints. In addition, the algorithm implements simple but effective adjustments such as presenting both positive and negative cues in each iteration to promote more efficient optimisation.

### 4. Issues addressed 
The algorithm addresses the problem of how to efficiently optimise the cue set in a low-sample image classification task. By interacting with ChatGPT and using its generated cues, the algorithm is able to quickly find high-quality cue sets, which significantly improves classification accuracy. Compared to other methods, the algorithm is not only more effective, but also more suitable for black-box scenarios as it does not require access to information such as pre-training weights, model architecture, feature embeddings or output logits.

![image](https://github.com/user-attachments/assets/e98c25a5-a270-416a-ae0f-e42822e0cb27)

## Experiments
This article focuses on the authors' use of a natural language cue-based optimisation framework for models such as CLIP and DALL-E, and explores the application of this approach to text-to-image generation tasks. Specifically, the article consists of the following three parts:

The first part presents experimental results **comparing the use of continuous cues with natural language cues**. The authors conducted experiments on the ImageNet dataset using the CoOp method as a benchmark. The results show that while the CoOp method performs better in the small sample case, the method with natural language cues has better generalisation performance in the large sample case.

![image](https://github.com/user-attachments/assets/46efb12b-cd90-4db6-861d-7cecd3928b05)

The second part focuses on **black-box cue transfer between CLIP architectures**. The authors tested this by transferring cues from the trained RN50 CLIP model to other CLIP architectures and found that natural language cues were able to maintain their performance and outperform the baseline, while the CoOp approach showed a significant degradation.

![image](https://github.com/user-attachments/assets/f312fc6d-0cd4-4bb0-85d5-1d22c5599e14)

The third part of the study is on **the application of natural language cues in text-to-image generation tasks**. The authors use a multimodal LLM consisting of DALL-E 3 and GPT4-V to provide feedback and optimise prompts. Through experiments, the authors demonstrate the effectiveness of this approach and the possibility of correcting DALL-E 3 errors in some complex user queries.

Overall, the paper presents an effective optimisation framework based on natural language cues and demonstrates its usefulness and superiority in several experiments. At the same time, the paper also points out the limitations of the method and suggests directions for future work.  

## Conclusion

### 1. Advantages of the Thesis
  1. This paper presents a method for optimising visual language models (VLAs) using a pre-trained language model that uses multiple rounds of dialogue to guide the language model in generating more effective cues, and achieves better results than human-designed cues on a single-image classification task.
  2. And, the method enhances the quality of image generation through natural language interpretive cues, allowing the user to have better control over the generated results. Also, the authors conducted a number of experiments to validate the effectiveness and scalability of the method.

### 2. Innovative points
  1. The main contribution of this paper is to propose a new approach to optimise VLAs, i.e., using pre-trained language models to generate more effective prompts. This approach not only improves the performance of VLAs, but also improves the quality of image generation by providing more control to the user.
  2. MOreover, the approach is highly scalable and can be improved with different CLIP backplanes and other powerful generation models. 

### 3. Future Works
Further future research should focus on solving these problems and applying the method to a wider range of scenarios to achieve higher performance and greater impact.  
